{"title":"HBase 读优化","slug":"yuque/HBase 读优化","date":"2021-06-20T16:00:00.000Z","updated":"2022-04-02T16:49:05.358Z","comments":true,"path":"api/articles/hbase/hbase-read-optimize.json","excerpt":"> https://blog.csdn.net/weixin_40954192/article/details/106942029LSM 存储引擎是在B+树的基础上衍生过来的，目的就是为了在读和写之间，提高写的性能。所以，LSM树的弊端也由此可见，对读并不是很友好，所以，针对LSM 树，有后续compact，布隆过滤器，blockCache等优化方式。来弥补对读的查询。LSM 树的索引一般由 2部分构成，一部分是内存部分，一部分是磁盘部分。内存部分采用跳跃表来维护一个有序的KV 集合,也就是memstore.随着内存不断数据写入，一旦内存占用超过一定的阈值，就把内存部分数","covers":["https://cdn.nlark.com/yuque/0/2022/png/104130/1647915437003-ca220200-1508-4b7f-9a2e-65f702f2d1e9.png#clientId=uc658cd9c-7178-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=263&id=uf54582c0&margin=%5Bobject%20Object%5D&name=image.png&originHeight=263&originWidth=1165&originalType=binary%E2%88%B6=1&rotation=0&showTitle=false&size=37515&status=done&style=none&taskId=ue4a68932-475c-49b0-9604-0ec8205fec1&title=&width=1165","https://cdn.nlark.com/yuque/0/2022/png/104130/1647915452495-be03b890-a16f-4960-9043-b7a11fcf17ec.png#clientId=uc658cd9c-7178-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=740&id=u1c1b950f&margin=%5Bobject%20Object%5D&name=image.png&originHeight=740&originWidth=1006&originalType=binary%E2%88%B6=1&rotation=0&showTitle=false&size=90458&status=done&style=none&taskId=ueac0d2b5-5c06-43c2-af65-786d4db2922&title=&width=1006","https://cdn.nlark.com/yuque/0/2022/png/104130/1647915475324-aaf3a59d-f6fe-4944-9012-5e22fc9149da.png#clientId=uc658cd9c-7178-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=791&id=u06632d08&margin=%5Bobject%20Object%5D&name=image.png&originHeight=791&originWidth=742&originalType=binary%E2%88%B6=1&rotation=0&showTitle=false&size=70335&status=done&style=none&taskId=u585d56ee-4dad-479e-b1dc-eb44883e88f&title=&width=742"],"content":"<blockquote>\n<p><a href=\"https://blog.csdn.net/weixin_40954192/article/details/106942029\">https://blog.csdn.net/weixin_40954192&#x2F;article&#x2F;details&#x2F;106942029</a></p>\n</blockquote>\n<p>LSM 存储引擎是在 B+树的基础上衍生过来的，目的就是为了在读和写之间，提高写的性能。所以，LSM 树的弊端也由此可见，对读并不是很友好，所以，针对 LSM 树，有后续 compact，布隆过滤器，blockCache 等优化方式。来弥补对读的查询。<br>LSM 树的索引一般由 2 部分构成，一部分是内存部分，一部分是磁盘部分。内存部分采用跳跃表来维护一个有序的 KV 集合,也就是 memstore.随着内存不断数据写入，一旦内存占用超过一定的阈值，就把内存部分数据进行导出（这里的 flush 操作实则是通过两个跳跃表来完成的），形成一个有序的数据文件，存储在磁盘上，磁盘部分则是对应的 hFile。</p>\n<h2 id=\"keyValue-存储格式\"><a href=\"#keyValue-存储格式\" class=\"headerlink\" title=\"keyValue 存储格式\"></a>keyValue 存储格式</h2><p>LSM 树中存储的是多个 keyValue 组成的集合。每一个 KeyValue 由一个字节数组来表示。如图<br><img src=\"https://cdn.nlark.com/yuque/0/2022/png/104130/1647915437003-ca220200-1508-4b7f-9a2e-65f702f2d1e9.png#clientId=uc658cd9c-7178-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=263&id=uf54582c0&margin=%5Bobject%20Object%5D&name=image.png&originHeight=263&originWidth=1165&originalType=binary%E2%88%B6=1&rotation=0&showTitle=false&size=37515&status=done&style=none&taskId=ue4a68932-475c-49b0-9604-0ec8205fec1&title=&width=1165\" alt=\"image.png\"></p>\n<h2 id=\"LSM-索引结构\"><a href=\"#LSM-索引结构\" class=\"headerlink\" title=\"LSM 索引结构\"></a>LSM 索引结构</h2><p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/104130/1647915452495-be03b890-a16f-4960-9043-b7a11fcf17ec.png#clientId=uc658cd9c-7178-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=740&id=u1c1b950f&margin=%5Bobject%20Object%5D&name=image.png&originHeight=740&originWidth=1006&originalType=binary%E2%88%B6=1&rotation=0&showTitle=false&size=90458&status=done&style=none&taskId=ueac0d2b5-5c06-43c2-af65-786d4db2922&title=&width=1006\" alt=\"image.png\">在 hbase 实现中，memstore 的数据达到某个级别的阈值之后，都会进行 flush 到 disk 中，形成一个 file。（前提为了怕 memstore 内存数据丢失，会先将数据写入到所属 regionServer 的 WAL 预写日志中）这个 file 的存储也就是一个小的 B+树，因为 hbase 一般是部署在 hdfs 上，hdfs 不支持对文件的 update 操作，而且最终随着磁盘文件越来越多，对读的影响很大。所以内存 flush 到磁盘上的小树，定期也会合并成一个大树。来增强读操作的性能，整体上 hbase 就是用了 lsm tree 的思路。</p>\n<h2 id=\"多路归并\"><a href=\"#多路归并\" class=\"headerlink\" title=\"多路归并\"></a>多路归并</h2><p>为了优化读取操作的性能，hbase 会进行两种类型的 compact。<br><strong>一种是 major Compact，是将所有的 HFile 一次性多路归并成一个文件</strong>。这种方式的好处是，合并之后只有一个文件，这样读取的性能肯定是最高的。但它的问题是合并所有的文件可能需要很长的时间并消耗大量的 IO 贷款，所以 major Compact 不宜使用太频繁，适合周期性的跑。或者我们手动设置在闲时合并。<br><strong>另一种是 minor Compact，即选中少数几个 Hfile，将他们多路归并成一个文件</strong>。这种方式的有点是可以进行局部的 Compact，通过少量的 IO 减少文件个数，提高读取操作的性能。适合较高频率的跑。但它的缺点是只合并了局部的数据，对于那些全局删除操作，无法在合并过程中完全删除。</p>\n<h2 id=\"多路归并原理\"><a href=\"#多路归并原理\" class=\"headerlink\" title=\"多路归并原理\"></a>多路归并原理</h2><p>比如现在我们有 K 个文件，其中第 i 个文件内存存储有 N 个正整数（这些整数在文件内按照从小到大的顺序排序）<br>多路归并的算法原理就是对每个文件设计一个指针，取出 K 个指针中数值最小的一个（对应的 K 个文件），然后把最小的那个指针后移，接着继续找 K 个指针中数值最小的一个，继续后移指针…直到文件全部读完为止。如图<br><img src=\"https://cdn.nlark.com/yuque/0/2022/png/104130/1647915475324-aaf3a59d-f6fe-4944-9012-5e22fc9149da.png#clientId=uc658cd9c-7178-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=791&id=u06632d08&margin=%5Bobject%20Object%5D&name=image.png&originHeight=791&originWidth=742&originalType=binary%E2%88%B6=1&rotation=0&showTitle=false&size=70335&status=done&style=none&taskId=u585d56ee-4dad-479e-b1dc-eb44883e88f&title=&width=742\" alt=\"image.png\"><br>针对读取操作，还涉及到了布隆过滤器。<br>布隆过滤器是由一个长度为 N 的 01 数组组成的。首先将数组 array 每个元素初始设为 0，对集合 A 中的每个元素 w，做 K 次哈希，第 i 次哈希值对 N 取模得到一个 index(i).即 index(i) &#x3D; Hash_i（w） % N,将 array 数组中的 array[index(i)] 置为 1.最终变为一个这些元素为 1 的 01 数组。<br><img src=\"https://cdn.nlark.com/yuque/0/2022/png/104130/1647915498955-975c2bea-6ff2-4e5d-af75-1169d740f0ce.png#clientId=uc658cd9c-7178-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=699&id=u7296dd92&margin=%5Bobject%20Object%5D&name=image.png&originHeight=699&originWidth=886&originalType=binary%E2%88%B6=1&rotation=0&showTitle=false&size=99911&status=done&style=none&taskId=u6465223c-5d78-4432-9628-51e703954a2&title=&width=886\" alt=\"image.png\">正是由于布隆过滤器只需占用极小的空间，便可给出 “可能存在” 和 “肯定不存在”的存在性判断。因此可以提前过滤掉很多不必要的数据块，从而节省了大量的磁盘 IO。hbase 的 get 操作就是通过运用低成本高效率的布隆过滤器来过滤大量无效数据块的，从而节省了大量磁盘 IO。</p>\n<p>如果在表中设置了 Bloomfilter，那么 HBase 会在生成 StoreFile 时包含一份 bloomfilter 结构的数据，称其为 MetaBlock；MetaBlock 与 DataBlock（真实的 KeyValue 数据）一起由 LRUBlockCache 维护。所以，开启 bloomfilter 会有一定的存储及内存 cache 开销。</p>\n<p>布隆过滤器的 3 中类型：</p>\n<ul>\n<li>none，关闭布隆过滤器功能</li>\n<li>row，按照 rowkey 计算布隆过滤器的二进制串并存储。get 查询时，必须带 rowkey.</li>\n<li>rowcol，按照 rowkey+family+qualifier 这 3 个字段拼出 byte[]来计算布隆过滤器值并存储。如果查询时，get 可以指定到这 3 个字段，则肯定可以通过布隆过滤器提高性能。</li>\n</ul>\n<p>任何类型的 get（基于 rowkey 或 row+col）Bloom Filter 的优化都能生效，关键是 get 的类型要匹配 Bloom Filter 的类型<br>基于 row 的 scan 是没办法走 Bloom Filter 的。因为<strong>Bloom Filter</strong>是需要事先知道过滤项的。对于顺序 scan 是没有事先办法知道 rowkey 的。而 get 是指明了 rowkey 所以可以用 Bloom Filter，scan 指明 column 同理。</p>\n<p>一般意义上的 scan 操作，是没法使用布隆过滤器提升性能的，因为布隆过滤器的 key 不确定。但是 row+col+qualify 的 scan 可以借助布隆过滤器去掉不存在此 qualify 的 storefile，也算是不错的优化了，而且指明 qualify 也能减少流量，因此 scan 尽量指明 qualify<br>关于<strong>BlockCache</strong>的内容较多，在后续文章补充。</p>\n","more":"<blockquote>\n<p><a href=\"https://blog.csdn.net/weixin_40954192/article/details/106942029\">https://blog.csdn.net/weixin_40954192&#x2F;article&#x2F;details&#x2F;106942029</a></p>\n</blockquote>\n<p>LSM 存储引擎是在 B+树的基础上衍生过来的，目的就是为了在读和写之间，提高写的性能。所以，LSM 树的弊端也由此可见，对读并不是很友好，所以，针对 LSM 树，有后续 compact，布隆过滤器，blockCache 等优化方式。来弥补对读的查询。<br>LSM 树的索引一般由 2 部分构成，一部分是内存部分，一部分是磁盘部分。内存部分采用跳跃表来维护一个有序的 KV 集合,也就是 memstore.随着内存不断数据写入，一旦内存占用超过一定的阈值，就把内存部分数据进行导出（这里的 flush 操作实则是通过两个跳跃表来完成的），形成一个有序的数据文件，存储在磁盘上，磁盘部分则是对应的 hFile。</p>\n<h2 id=\"keyValue-存储格式\"><a href=\"#keyValue-存储格式\" class=\"headerlink\" title=\"keyValue 存储格式\"></a>keyValue 存储格式</h2><p>LSM 树中存储的是多个 keyValue 组成的集合。每一个 KeyValue 由一个字节数组来表示。如图<br><img src=\"https://cdn.nlark.com/yuque/0/2022/png/104130/1647915437003-ca220200-1508-4b7f-9a2e-65f702f2d1e9.png#clientId=uc658cd9c-7178-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=263&id=uf54582c0&margin=%5Bobject%20Object%5D&name=image.png&originHeight=263&originWidth=1165&originalType=binary%E2%88%B6=1&rotation=0&showTitle=false&size=37515&status=done&style=none&taskId=ue4a68932-475c-49b0-9604-0ec8205fec1&title=&width=1165\" alt=\"image.png\"></p>\n<h2 id=\"LSM-索引结构\"><a href=\"#LSM-索引结构\" class=\"headerlink\" title=\"LSM 索引结构\"></a>LSM 索引结构</h2><p><img src=\"https://cdn.nlark.com/yuque/0/2022/png/104130/1647915452495-be03b890-a16f-4960-9043-b7a11fcf17ec.png#clientId=uc658cd9c-7178-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=740&id=u1c1b950f&margin=%5Bobject%20Object%5D&name=image.png&originHeight=740&originWidth=1006&originalType=binary%E2%88%B6=1&rotation=0&showTitle=false&size=90458&status=done&style=none&taskId=ueac0d2b5-5c06-43c2-af65-786d4db2922&title=&width=1006\" alt=\"image.png\">在 hbase 实现中，memstore 的数据达到某个级别的阈值之后，都会进行 flush 到 disk 中，形成一个 file。（前提为了怕 memstore 内存数据丢失，会先将数据写入到所属 regionServer 的 WAL 预写日志中）这个 file 的存储也就是一个小的 B+树，因为 hbase 一般是部署在 hdfs 上，hdfs 不支持对文件的 update 操作，而且最终随着磁盘文件越来越多，对读的影响很大。所以内存 flush 到磁盘上的小树，定期也会合并成一个大树。来增强读操作的性能，整体上 hbase 就是用了 lsm tree 的思路。</p>\n<h2 id=\"多路归并\"><a href=\"#多路归并\" class=\"headerlink\" title=\"多路归并\"></a>多路归并</h2><p>为了优化读取操作的性能，hbase 会进行两种类型的 compact。<br><strong>一种是 major Compact，是将所有的 HFile 一次性多路归并成一个文件</strong>。这种方式的好处是，合并之后只有一个文件，这样读取的性能肯定是最高的。但它的问题是合并所有的文件可能需要很长的时间并消耗大量的 IO 贷款，所以 major Compact 不宜使用太频繁，适合周期性的跑。或者我们手动设置在闲时合并。<br><strong>另一种是 minor Compact，即选中少数几个 Hfile，将他们多路归并成一个文件</strong>。这种方式的有点是可以进行局部的 Compact，通过少量的 IO 减少文件个数，提高读取操作的性能。适合较高频率的跑。但它的缺点是只合并了局部的数据，对于那些全局删除操作，无法在合并过程中完全删除。</p>\n<h2 id=\"多路归并原理\"><a href=\"#多路归并原理\" class=\"headerlink\" title=\"多路归并原理\"></a>多路归并原理</h2><p>比如现在我们有 K 个文件，其中第 i 个文件内存存储有 N 个正整数（这些整数在文件内按照从小到大的顺序排序）<br>多路归并的算法原理就是对每个文件设计一个指针，取出 K 个指针中数值最小的一个（对应的 K 个文件），然后把最小的那个指针后移，接着继续找 K 个指针中数值最小的一个，继续后移指针…直到文件全部读完为止。如图<br><img src=\"https://cdn.nlark.com/yuque/0/2022/png/104130/1647915475324-aaf3a59d-f6fe-4944-9012-5e22fc9149da.png#clientId=uc658cd9c-7178-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=791&id=u06632d08&margin=%5Bobject%20Object%5D&name=image.png&originHeight=791&originWidth=742&originalType=binary%E2%88%B6=1&rotation=0&showTitle=false&size=70335&status=done&style=none&taskId=u585d56ee-4dad-479e-b1dc-eb44883e88f&title=&width=742\" alt=\"image.png\"><br>针对读取操作，还涉及到了布隆过滤器。<br>布隆过滤器是由一个长度为 N 的 01 数组组成的。首先将数组 array 每个元素初始设为 0，对集合 A 中的每个元素 w，做 K 次哈希，第 i 次哈希值对 N 取模得到一个 index(i).即 index(i) &#x3D; Hash_i（w） % N,将 array 数组中的 array[index(i)] 置为 1.最终变为一个这些元素为 1 的 01 数组。<br><img src=\"https://cdn.nlark.com/yuque/0/2022/png/104130/1647915498955-975c2bea-6ff2-4e5d-af75-1169d740f0ce.png#clientId=uc658cd9c-7178-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=699&id=u7296dd92&margin=%5Bobject%20Object%5D&name=image.png&originHeight=699&originWidth=886&originalType=binary%E2%88%B6=1&rotation=0&showTitle=false&size=99911&status=done&style=none&taskId=u6465223c-5d78-4432-9628-51e703954a2&title=&width=886\" alt=\"image.png\">正是由于布隆过滤器只需占用极小的空间，便可给出 “可能存在” 和 “肯定不存在”的存在性判断。因此可以提前过滤掉很多不必要的数据块，从而节省了大量的磁盘 IO。hbase 的 get 操作就是通过运用低成本高效率的布隆过滤器来过滤大量无效数据块的，从而节省了大量磁盘 IO。</p>\n<p>如果在表中设置了 Bloomfilter，那么 HBase 会在生成 StoreFile 时包含一份 bloomfilter 结构的数据，称其为 MetaBlock；MetaBlock 与 DataBlock（真实的 KeyValue 数据）一起由 LRUBlockCache 维护。所以，开启 bloomfilter 会有一定的存储及内存 cache 开销。</p>\n<p>布隆过滤器的 3 中类型：</p>\n<ul>\n<li>none，关闭布隆过滤器功能</li>\n<li>row，按照 rowkey 计算布隆过滤器的二进制串并存储。get 查询时，必须带 rowkey.</li>\n<li>rowcol，按照 rowkey+family+qualifier 这 3 个字段拼出 byte[]来计算布隆过滤器值并存储。如果查询时，get 可以指定到这 3 个字段，则肯定可以通过布隆过滤器提高性能。</li>\n</ul>\n<p>任何类型的 get（基于 rowkey 或 row+col）Bloom Filter 的优化都能生效，关键是 get 的类型要匹配 Bloom Filter 的类型<br>基于 row 的 scan 是没办法走 Bloom Filter 的。因为<strong>Bloom Filter</strong>是需要事先知道过滤项的。对于顺序 scan 是没有事先办法知道 rowkey 的。而 get 是指明了 rowkey 所以可以用 Bloom Filter，scan 指明 column 同理。</p>\n<p>一般意义上的 scan 操作，是没法使用布隆过滤器提升性能的，因为布隆过滤器的 key 不确定。但是 row+col+qualify 的 scan 可以借助布隆过滤器去掉不存在此 qualify 的 storefile，也算是不错的优化了，而且指明 qualify 也能减少流量，因此 scan 尽量指明 qualify<br>关于<strong>BlockCache</strong>的内容较多，在后续文章补充。</p>\n","categories":[{"name":"hbase","path":"api/categories/hbase.json"}],"tags":[{"name":"HBase","path":"api/tags/HBase.json"}]}